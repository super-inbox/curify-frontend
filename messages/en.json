{
    "home": {
    "hero": {
      "title": "Power Content Creation with AI",
      "description": "Curify Studio is building an AI-native content creation platform that empowers creators and organizations to overcome language and format barriers. We solve the challenge of scaling content across global audiences, enabling authentic translations that preserve tone, style, and emotional depth. Operating at the intersection of media, education, and entertainment, we provide tools for creators to adapt content seamlessly in a rapidly globalizing industry."
    }
  },
  "coreFeatures": {
  "oneShot": {
    "title": "One-Shot Translation",
    "desc": "Complete video translation with voice-over, subtitles, and lip sync in a single process."
  },
  "toneColor": {
    "title": "Tone Color Preservation",
    "desc": "Maintains the original speaker's unique voice characteristics and tonal qualities."
  },
  "emotional": {
    "title": "Emotional Speech",
    "desc": "AI reproduces emotional nuances, ensuring authentic expression across languages."
  },
  "lipSync": {
    "title": "Lip Sync Technology",
    "desc": "Advanced lip synchronization that perfectly matches mouth movements to translated audio."
  },
  "subtitle": {
    "title": "Subtitle Captioner",
    "desc": "Intelligent subtitle generation with precise timing and natural language flow."
  },
  "languages": {
    "title": "170+ Languages",
    "desc": "Translate your content into over 170 languages with native-level accuracy."
  
  
  }
},
  "upcoming": {
    "title": "Coming Soon",
    "subtitle": "Next-generation features in development",
    "styleTransfer": {
      "title": "Style Transfer",
      "desc": "Apply cinematic or brand-specific styles to your videos with AI-powered visual transformation.",
      "icon": "üé®",
      "status": "Coming Q3 2025",
      "transcript": "This demo transforms a live-action drama clip into a whimsical Ghibli-style animation using AI-driven visual style transfer."
    },
    "mangaTranslation": {
      "title": "Manga Translation",
      "desc": "Automated manga and comic translation with text detection, bubble editing, and cultural adaptation.",
      "icon": "üìö",
      "status": "Coming Q3 2025",
      "transcript": "This prototype demonstrates automatic manga translation, including speech bubble detection and bilingual in-place editing."
    },
    "templatedVideo": {
      "title": "Templated Video Generation",
      "desc": "Create professional videos from templates with AI-generated content and custom branding.",
      "icon": "üé¨",
      "status": "Coming Q4 2025",
      "transcript": "This demo shows a scripted-to-video pipeline generating a historical scene about early U.S. history by planning out the storyboard and assembling visuals."
    },
    "statusQ3": "Coming in Q3 2025",
    "statusQ4": "Coming in Q4 2025"
  },
  "export": {
    "title": "Export",
    "export": "Export",
    "downloading": "Downloading..."
  },
  "delete": {
    "title": "Double Confirm",
    "message": "Are you sure you want to delete this project",
    "warning": "This action cannot be undone.",
    "cancel": "Cancel",
    "delete": "Delete",
    "deleting": "Deleting..."
  },
  "userMenu": {
    "topUpCredits": "Top Up Credits",
    "remaining": "Remaining",
    "planRemaining": "Plan Remaining", 
    "validUntil": "Valid until",
    "creditsHistory": "Credits History",
    "subscribePlan": "Subscribe Plan",
    "supportTicket": "Support Ticket",
    "signOut": "Sign Out"
  },
  "technology": {
    "multimodal": {
      "title": "Multi-modal Recognition",
      "desc": "We combine speech and subtitle signals to achieve more accurate and robust transcription. This dual-channel recognition reduces errors and ensures better alignment with the original content, especially in noisy or complex audio scenes."
    },
    "emotional": {
      "title": "Emotional Speech",
      "desc": "Our voice synthesis models generate expressive, emotionally rich speech that enhances storytelling and viewer engagement. By capturing tone, rhythm, and nuance, we make AI voices feel more human and relatable."
    },
    "lengthaware": {
      "title": "Length-aware Translation and Customization",
      "desc": "We optimize translations not just for accuracy, but also for timing and pacing‚Äîcrucial for video and voice alignment. Users can further customize tone, length, and phrasing to suit different content needs or audience preferences."
    },
    "controlled": {
      "title": "Controlled Video Generation",
      "desc": "We enable structured, template-driven video generation with controllable visual elements and transitions. This gives creators both creative freedom and production consistency, reducing manual effort while ensuring high-quality output."
    }
  },
  "tools": {
    "video_dubbing": {
      "title": "Video Dubbing",
      "desc": "Translate your video into any language with accurate localization and voice sync"
    },
    "subtitle_captioner": {
      "title": "Subtitle Captioner",
      "desc": "Auto-generate multilingual subtitles to enhance clarity and accessibility"
    },
    "lip_syncing": {
      "title": "Lip Syncing",
      "desc": "Match lips to speech perfectly with AI-powered lip sync"
    },
    "style_transfer": {
      "title": "Style Transfer",
      "desc": "Transform your video into Pixar, Ghibli, or other artistic styles ‚Äî coming soon"
    },
    "coming_soon": "Coming Soon",
    "create": "Create"
  },
  "bilingual": {
  "title": "Bilingual Subtitles Made Easy",
  "intro": "Automatically generate subtitles in two languages using Curify‚Äôs AI subtitle engine. Ideal for creators, educators, and global businesses.",
  "example": "Example: Jensen Huang explaining AI strategy ‚Äî English + Chinese subtitles auto‚Äëgenerated.",
  "cta": "Try It Free",
  "why": {
    "title": "Why Choose Curify for Bilingual Subtitles?",
    "point1": "Supports over 170 languages with accurate translation.",
    "point2": "Preserves emotional tone and timing.",
    "point3": "Perfect for YouTube, TikTok, and educational videos.",
    "point4": "Export ready‚Äëto‚Äëpublish subtitle files or embedded videos."
  },
  "faq": {
    "title": "Frequently Asked Questions",
    "q1": "Is it really free?",
    "a1": "Yes, Curify offers a free plan with limited subtitle minutes per month.",
    "q2": "Can I edit subtitles after generation?",
    "a2": "Absolutely. You can adjust timing, translation, and style before exporting."
  }
},
"videoDubbing": {
  "title": "AI Video Dubbing with Natural Voice",
  "description": "Translate and dub your video content into over 170 languages with realistic AI voice and lip sync. Perfect for international creators and brands."
},
"pricing": {
  "header": {
    "title": "Getting started with a plan",
    "subtitle": "Choose a plan that fits your needs."
  },
  "common": {
    "month": "Month",
    "pricing": "Pricing",
    "receive": "Receive"
  },
  "buttons": {
    "signUp": "Sign up",
    "subscribePlan": "Subscribe Plan",
    "currentPlan": "Current Plan",
    "comingSoon": "Coming Soon",
    "downgradeToFree": "Downgrade to Free",
    "downgradeToCreator": "Downgrade to Creator",
    "contactSales": "Contact Sales"
  },
  "plans": {
    "free": {
      "name": "Free",
      "description": "Great for testing tools and light work",
      "features": [
        "Download videos without watermark",
        "1 hour free subtitle processing",
        "Export SRT files"
      ]
    },
    "creator": {
      "name": "Creator",
      "description": "Ideal for small creators doing regular subtitling and light dubbing",
      "plusTitle": "Everything in Free, plus:",
      "features": [
        "Lip Syncing",
        "Batch processing",
        "5 hours free subtitle processing",
        "Up to 30 minutes per task"
      ]
    },
    "pro": {
      "name": "Pro",
      "description": "Coming Soon ‚Äî Designed for high-volume creators",
      "plusTitle": "Everything in Creator, plus:",
      "features": [
        "Voice beautification & noise removal",
        "Priority queue",
        "Up to 60 min per task",
        "Unlimited subtitle minutes"
      ]
    },
    "enterprise": {
      "name": "Enterprise",
      "description": "For teams, studios, and enterprise workflows",
      "customPricing": "Custom",
      "unlimited": "Unlimited üêö",
      "tailoredSupport": "& tailored support",
      "plusTitle": "Everything in Pro, plus:",
      "features": [
        "Dedicated account manager",
        "On-premise deployment options",
        "API access & usage analytics",
        "Custom integrations & SLA",
        "Unlimited processing hours"
      ]
    }
  },
  "table": {
    "header": {
      "feature": "Feature / Limit"
    },
    "rows": {
      "videoDownloadWithWatermark": "Video Download with Watermark",
      "videoDownloadWithoutWatermark": "Video Download without Watermark",
      "downloadSrt": "Download SRT File",
      "voiceBeautification": "Voice Beautification & Noise Removal",
      "lipSync": "Lip Sync",
      "subtitleTools": "Subtitle Tools (add/remove, no translation)",
      "batchProcessing": "Batch Processing",
      "maxVideoLength": "Max Video Length per Task",
      "monthlyCredits": "Monthly Free Credits",
      "priorityQueue": "Priority Queue / Faster Processing",
      "creditTopUp": "Credit Top-up"
    },
    "values": {
      "free": "Free",
      "unlimited": "Unlimited",
      "oneHourPerMonth": "1 hrs/month",
      "fiveHoursPerMonth": "5 hrs/month",
      "fiveMinutes": "5 min",
      "thirtyMinutes": "30 min",
      "sixtyMinutes": "60 min"
    }
  }
},
"aeVsComfyUi": {
  "title": "AE vs ComfyUI ‚Äì Redefining Animation (Part 3)",
  "heading": "üé® AE vs ComfyUI",
  "intro1": "We often get asked: why not just use After Effects (AE)? Or why bother with ComfyUI? Both are essential ‚Äî one provides fine control, the other generative creativity.",
  "intro2": "AE offers timeline precision and control over transitions, lighting, and effects. ComfyUI brings AI-native workflows, combining diffusion, text, and multimodal orchestration.",
  "table": {
    "colAE": "AE",
    "colComfy": "ComfyUI",
    "row1a": "Precise VFX and motion control",
    "row1b": "AI-native generation of videos",
    "row2a": "Timeline-based editing",
    "row2b": "Workflow/node-based rendering",
    "row3a": "Manual / plugin-driven workflow",
    "row3b": "Diffusion + LLM + multimodal orchestration"
  },
  "outro": "In our workflow, ComfyUI handles the AI generation and iteration, while AE refines the final look ‚Äî transitions, overlays, and compositing."
},
"video_translation_eval":{
  "title": "Evaluating AI Video Translation Quality",
  "subtitle": "Metrics that Matter",
  "intro": "Translating videos across languages is no small feat ‚Äî it involves transcription, translation, voice synthesis, timing, and more. At Curify, we‚Äôve built a robust evaluation pipeline to ensure each piece meets industry standards.",
  "section1_title": "1. Transcription Quality",
  "section1_engine": "Engine: WhisperX",
  "section1_metrics": ["WER (Word Error Rate)", "Punctuation F1 (for expressiveness and readability)"],
  "section2_title": "2. Translation Quality",
  "section2_engine": "Engines: Helsinki, MarianMT",
  "section2_metrics": ["BLEU (standard metric)", "COMET / chrF++ (semantic similarity)", "Human review: fluency + adequacy"],
  "section3_title": "3. Voice Synthesis Quality",
  "section3_engine": "Engines: XTTS / YourTTS",
  "section3_metrics": ["MOS (Naturalness, similarity, expressiveness)", "Speaker verification accuracy"],
  "section4_title": "4. Alignment & Lip Sync",
  "section4_metrics": ["Segment duration mismatch", "Wav2Lip sync confidence", "Temporal drift analysis"],
  "section5_title": "5. Semantic Preservation",
  "section5_method": "We use LLMs (like GPT-4) to judge whether the translated speech preserves the original meaning, tone, and emotion. Example prompt:",
  "section5_prompt": "Compare this Mandarin transcript to the English voiceover. Does the tone, intent, and content match? Rate 1‚Äì5 and explain.",
  "section6_title": "6. User Feedback & GTM Validation",
  "section6_metrics": ["Voice quality fit for product category", "Viewer retention improvement", "Adoption willingness from early users (e.g., 1688 sellers)"],
  "closing": "At Curify, we don‚Äôt just translate ‚Äî we preserve storytelling, emotion, and clarity across languages. If you‚Äôre ready to scale your voice globally, get in touch.",
  "available_locales": "Post available in: [EN] [ZH] [ES] [DE]"
},

"storyboardToPipeline": {
  "title": "From Storyboards to AI Pipelines ‚Äì Redefining Animation",
  "intro": {
    "p1": "Most people think AI video means \"text in, clip out.\" But if you're aiming for <strong>cinematic, director-level control</strong>, it's an entirely different game.",
    "p2": "In traditional animation, every detail matters ‚Äî character design, motion continuity, timing, and scene transitions. Our goal is to make AI match that level of precision.",
    "p3": "Animation today is both an art and a structured orchestration challenge. We think like directors, but build like engineers.",
    "p4": "That's why we build <strong>Controlled Generation Pipelines</strong> instead of one-shot generation. These pipelines combine structure and creativity:",
    "pipelineItems": [
      "Storyboard-to-Video: JSON-based scene specs with IDs, timing, and camera metadata",
      "ComfyUI-based workflows: node-graph DAGs with versioned checkpoints, LoRA stacks, and seeds",
      "Temporal & multimodal control: shared scene IDs across image, motion, audio, and subtitles"
    ],
    "p5": "Now, let's walk through a simple example to show how AI pipelines work in practice.",
    "technicalNotes": [
      "All stages read/write typed artifacts (JSON/CSV, PNG sequences, WAV/MP3, MP4, .aep)",
      "Pipelines are orchestrated via Python/CLI (e.g., ComfyUI API, ffmpeg, TTS APIs)",
      "Every run is reproducible through stored config (model, sampler, seed, prompts, duration)"
    ]
  },
  "pipeline": {
    "title": "AI Video Generation Pipeline",
    "description": "The AI video generation pipeline transforms text prompts into polished videos through structured stages with explicit inputs, outputs, and configs.",
    "prompt": "1. Prompt (raw idea ‚Üí structured JSON spec)",
    "storyboard": "2. Storyboard (scene/shot table with timing, camera, and description)",
    "images": "3. Images (per-shot keyframes generated via Stable Diffusion / ComfyUI)",
    "animation": "4. Animation (image sequences ‚Üí motion, parallax, and effects)",
    "voiceOver": "5. Voice Over (TTS + alignment data)",
    "finalVideo": "6. Final Video (ffmpeg composition: video + audio + subtitles)",
    "features": [
      "JSON-first design: every scene is addressable and scriptable (scene_id, shot_id)",
      "ComfyUI-based workflows: modular, reproducible, composable DAGs for image/video generation",
      "Temporal & multimodal control: consistent seeds, character embeddings, and timing across modalities"
    ],
    "exampleIntro": "Now, let's walk through a simple example to show how AI pipelines work in practice.",
    "artifacts": {
      "promptSpec": "prompt_run_001.json",
      "storyboardTable": "storyboard_v1.csv",
      "imageOutputDir": "renders/scene_{scene_id}/shot_{shot_id}/frame_####.png",
      "audioOutput": "audio/voiceover_final.wav",
      "subtitleFile": "subtitles/storyboard_v1.srt",
      "finalVideoFile": "exports/storyboard_v1_final.mp4"
    }
  },
  "steps": {
    "step1": {
      "title": "Step 1: Start with a Basic Prompt",
      "example": "A girl stands at a midnight train station, wind blowing her hair.",
      "expandedPrompt": "With the help of GPT or a local LLM, we expand this into a structured JSON object with global style, character definitions, and per-scene breakdown.",
      "technicalDetails": {
        "llmFields": [
          "global_style: art style, camera language, color palette",
          "characters: name, age, look, outfit, emotional state",
          "scenes: [{ scene_id, shot_id, description, camera_type, duration_seconds }]"
        ],
        "outputFile": "prompt_run_001.json",
        "exampleStack": "Python script calling OpenAI/LLM API and writing JSON to disk"
      }
    },
    "step2": {
      "title": "Step 2: Convert Prompt to a Storyboard Table",
      "description": "The JSON spec is flattened into a storyboard table (CSV or Notion) that both humans and the pipeline can read.",
      "technicalDetails": {
        "columns": [
          "scene_id",
          "shot_id",
          "start_time",
          "end_time",
          "duration_seconds",
          "camera_type",
          "visual_description",
          "character_state",
          "motion_notes"
        ],
        "storage": "storyboard_v1.csv (used as the single source of truth for downstream steps)",
        "automation": "Python / Node.js script transforms prompt_run_001.json ‚Üí storyboard_v1.csv"
      }
    },
    "step3": {
      "title": "üõ†Ô∏è Step 3: Generate Visuals",
      "description": "Generate high-quality keyframe images for each shot using Stable Diffusion through a ComfyUI workflow.",
      "points": [
        "üé® Use <strong>Stable Diffusion</strong> or <strong>ComfyUI</strong> to turn each row in `storyboard_v1.csv` into a high-res keyframe.",
        "Keep the style consistent by using the same base checkpoint, LoRA stack, sampler, and seed policy across all shots.",
        "Refine images with inpainting (for faces/hands) and outpainting (for extended compositions and camera motion)."
      ],
      "technicalDetails": {
        "modelConfig": {
          "checkpoint": "sd_xl_base_1.0.safetensors",
          "vae": "sdxl_vae.safetensors",
          "sampler": "euler_a",
          "steps": 25,
          "cfgScale": 6.5,
          "resolution": "1024x576",
          "loraStack": [
            "character_girl_v2.safetensors",
            "style_cinematic_blue_hour.safetensors"
          ]
        },
        "consistency": {
          "seedStrategy": "fixed_per_scene",
          "seedExample": "seed = hash(scene_id + character_name)",
          "namingConvention": "renders/scene_{scene_id}/shot_{shot_id}/kf_0001.png"
        },
        "comfyui": {
          "workflowFile": "workflows/storyboard_to_frame.json",
          "inputMethod": "custom node / HTTP API reading storyboard_v1.csv and modelConfig",
          "batching": "GPU batches keyed by scene_id to reuse loaded weights"
        }
      }
    },
    "step4": {
      "title": "üé¨ Step 4: Add Motion and Atmosphere in After Effects",
      "description": "Enhance static keyframes with motion, parallax, and atmosphere using Adobe After Effects (or an equivalent compositor).",
      "points": [
        "Import image sequences or keyframes into <strong>Adobe After Effects</strong> as layered compositions.",
        "Apply keyframe animations: pan, zoom, parallax layers, fog overlays, glow and light flicker.",
        "Add ambient sound cues and cinematic transitions between scenes."
      ],
      "technicalDetails": {
        "input": "renders/scene_{scene_id}/shot_{shot_id}/kf_0001.png (plus optional depth/alpha maps)",
        "projectFile": "ae/storyboard_v1.aep",
        "automation": [
          "Use AE scripts/ExtendScript or JSX to auto-create comps per scene_id.",
          "Map `duration_seconds` from storyboard_v1.csv ‚Üí comp duration.",
          "Generate intermediate frames via AE plugins or video-aware diffusion tools if needed."
        ],
        "output": "ae_renders/scene_{scene_id}_shot_{shot_id}_anim.mp4"
      }
    },
    "step5": {
      "title": "üéß Step 5: Add Voice and Subtitles",
      "description": "Generate voiceover aligned to the storyboard and attach subtitles for accessibility and clarity.",
      "points": [
        "Use <strong>XTTS</strong> or <strong>ElevenLabs</strong> to generate natural voiceovers from the script, using a consistent speaker profile.",
        "For acronyms (like API, NBA), generate English snippets separately and merge in post to keep pronunciation clean.",
        "Add subtitles using `.srt` or `.json` timeline files synced to the voiceover track."
      ],
      "technicalDetails": {
        "scriptSource": "dialogue and narration pulled from prompt_run_001.json + storyboard_v1.csv",
        "ttsConfig": {
          "engine": "XTTS or ElevenLabs",
          "voiceId": "cinematic_female_en",
          "sampleRate": 44100,
          "format": "wav"
        },
        "outputs": {
          "voiceFile": "audio/storyboard_v1_voiceover.wav",
          "subtitleFile": "subtitles/storyboard_v1.srt"
        },
        "syncStrategy": "align `start_time` / `end_time` from storyboard_v1.csv with subtitle cues; ensure total audio length ‚âà final timeline duration"
      }
    },
    "step6": {
      "title": "Step 6: Final Composition with FFMPEG",
      "description": "Use FFMPEG to combine all pieces into one final video file with audio and subtitles.",
      "technicalDetails": {
        "inputs": {
          "video": "ae_renders/storyboard_v1_timeline.mp4",
          "audio": "audio/storyboard_v1_voiceover.wav",
          "subtitles": "subtitles/storyboard_v1.srt"
        },
        "ffmpegExample": "ffmpeg -i storyboard_v1_timeline.mp4 -i storyboard_v1_voiceover.wav -c:v libx264 -c:a aac -shortest -vf subtitles=subtitles/storyboard_v1.srt exports/storyboard_v1_final.mp4",
        "output": "exports/storyboard_v1_final.mp4 (H.264, 24 fps, AAC audio)"
      }
    }
  },
  "whatYouNeed": "üìÅ What You'll Need",
  "whatYouNeedTechnical": [
    "GPU with at least 12GB VRAM for Stable Diffusion / ComfyUI",
    "Python environment for JSON/CSV transforms and automation scripts",
    "ComfyUI installed with required custom nodes and workflows",
    "Adobe After Effects or similar compositor for motion and FX",
    "XTTS / ElevenLabs API access for TTS",
    "ffmpeg installed and available on the command line"
  ],
  "cta": "üöÄ Ready to bring your storyboard to life with AI? <strong>We can provide a full starter kit</strong> with example JSONs, ComfyUI workflows, and ffmpeg/AE templates to help you get started."
},

  "SceneDetection": {
    "header": {
      "title": "From Raw Footage to Storyboards",
      "author": "Curify AI Team",
      "role": "AI Research Team",
      "imageAlt": "AI analyzing video scenes and generating storyboards",
      "imageCaption": "AI-powered scene detection in action, identifying key moments in video content",
      "subtitle": "AI-Powered Video Analysis",
      "date": "December 11, 2025",
      "readingTime": "8 min read"
    },
    "intro": {
      "p1": "In today's fast-paced content creation landscape, <strong>efficient video analysis</strong> is no longer a luxury‚Äîit's a necessity. Our AI-powered scene detection technology transforms hours of raw footage into structured, searchable content in minutes, not days.",
      "p2": "Whether you're a filmmaker, content creator, or media professional, understanding the power of automated scene detection can revolutionize your workflow and unlock new creative possibilities."
    },
    "proTip": "For best results, ensure your source video has clear audio and visual separation between scenes. Well-lit environments and minimal background noise significantly improve detection accuracy.",
    "process": {
      "title": "How Our Scene Detection Works",
      "steps": {
        "step1": {
          "title": "1. Video Analysis",
          "description": "Our AI scans your video frame by frame, analyzing visual and audio cues to identify potential scene boundaries.",
          "details": [
            "Analyzes color histograms and motion vectors",
            "Detects audio level changes and silences",
            "Identifies shot boundaries and transitions"
          ]
        },
        "step2": {
          "title": "2. Scene Segmentation",
          "description": "The system groups related shots into coherent scenes based on visual and temporal relationships.",
          "details": [
            "Uses machine learning to understand scene context",
            "Considers shot duration and transition types",
            "Merges related shots into logical scenes"
          ]
        },
        "step3": {
          "title": "3. Content Classification",
          "description": "Each detected scene is analyzed and categorized based on its visual and audio content.",
          "details": [
            "Identifies key visual elements and objects",
            "Classifies scene type (e.g., interview, action, landscape)",
            "Analyzes audio for speech, music, and effects"
          ]
        },
        "step4": {
          "title": "4. Metadata Generation",
          "description": "Comprehensive metadata is generated for each scene, enabling powerful search and organization.",
          "details": [
            "Creates timestamps and duration information",
            "Generates representative thumbnails",
            "Extracts key dialogue and audio cues"
          ]
        }
      }
    },
    "whyItWorks": {
      "title": "Why Our Scene Detection Works So Well",
      "points": "<li>Our AI analyzes both <strong>visual and audio cues</strong> to identify scene boundaries with over 95% accuracy.</li><li>Advanced <strong>machine learning models</strong> understand context, not just visual changes, for more natural scene detection.</li><li>Real-time processing allows for <strong>immediate feedback</strong> and adjustments during filming or editing.</li>"
    },
    "recommendedStyle": {
      "title": "Recommended Style for Best Results",
      "description": "To get the most accurate scene detection, we recommend the following style guidelines:",
      "points": [
        "Maintain consistent lighting within each scene",
        "Use clear audio markers or clapperboards between takes",
        "Keep camera movements smooth and intentional",
        "Maintain consistent audio levels within scenes"
      ]
    },
    "title": "From Raw Footage to Storyboards: AI-Powered Video Analysis",
    "description": "Discover how AI transforms raw video into structured storyboards with automated scene detection and analysis.",
    "timeline": {
      "title": "Scene Timeline",
      "viewScene": "View Scene {1}"
    },
    "cta": "üöÄ Ready to bring your storyboard to life with AI? <strong>We can provide a full starter kit</strong> with example JSONs, ComfyUI workflows, and ffmpeg/AE templates to help you get started.",
    "sceneBreakdown": {
      "title": "Understanding Scene Detection Output",
      "introduction": "Let's break down a typical scene detection output to understand how our AI analyzes and structures video content. Below each explanation, you'll find the corresponding JSON structure that powers these insights.",
      "scenes": [
        {
          "title": "1. Scene Identification",
          "content": "Each scene is assigned a unique identifier and timestamp range, allowing for precise navigation through the video content. This forms the foundation of our analysis.",
          "example": "Scene 1 (00:00:02.50 - 00:00:05.20)",
          "jsonExample": {
            "scene_id": "scene_001",
            "start_time": 2.5,
            "end_time": 5.2,
            "duration": 2.7,
            "keyframe_index": 5,
            "keyframe_time": 3.8
          },
          "jsonDescription": "This JSON structure shows the basic identification data for a scene, including its unique ID, timing information, and the index/time of its representative keyframe."
        },
        {
          "title": "2. Visual Analysis",
          "content": "Our AI examines keyframes to understand the visual composition of each scene, including dominant colors, lighting conditions, and visual elements.",
          "example": "Keyframe analysis: Outdoor, daylight, multiple subjects",
          "jsonExample": {
            "visual_analysis": {
              "brightness": 0.78,
              "contrast": 0.65,
              "color_palette": ["#3A5FCD", "#87CEEB", "#F5F5DC"],
              "dominant_colors": [
                {"color": "#3A5FCD", "percentage": 0.45},
                {"color": "#87CEEB", "percentage": 0.35},
                {"color": "#F5F5DC", "percentage": 0.20}
              ],
              "lighting_condition": "daylight",
              "environment": "outdoor",
              "detected_objects": [
                {"label": "person", "confidence": 0.97, "count": 2},
                {"label": "sky", "confidence": 0.99, "count": 1}
              ]
            }
          },
          "jsonDescription": "This JSON shows the visual analysis data, including color information, lighting conditions, and detected objects with confidence scores."
        },
        {
          "title": "3. Shot Composition",
          "content": "Within each scene, we identify individual shots and their transitions, helping understand the visual flow and pacing of the content.",
          "example": "3 shots detected with smooth cuts and one cross-fade",
          "jsonExample": {
            "shots": [
              {
                "shot_id": "shot_001",
                "start_time": 2.5,
                "end_time": 3.1,
                "transition": {
                  "type": "cut",
                  "confidence": 0.98
                },
                "camera_motion": {
                  "type": "static",
                  "confidence": 0.92
                }
              },
              {
                "shot_id": "shot_002",
                "start_time": 3.1,
                "end_time": 4.3,
                "transition": {
                  "type": "fade",
                  "duration": 0.3,
                  "confidence": 0.95
                },
                "camera_motion": {
                  "type": "pan_left",
                  "confidence": 0.88
                }
              }
            ]
          },
          "jsonDescription": "This JSON structure details the shot composition within a scene, including timing, transition types, and camera motion analysis."
        },
        {
          "title": "4. Content Classification",
          "content": "Scenes are automatically categorized based on their content, making it easy to find specific types of footage later.",
          "example": "Category: Interview, Setting: Studio, Subjects: 2",
          "jsonExample": {
            "content_analysis": {
              "primary_category": "interview",
              "secondary_categories": ["conversation", "talking_head"],
              "setting": {
                "type": "studio",
                "confidence": 0.94
              },
              "subjects": [
                {
                  "type": "person",
                  "count": 2,
                  "positions": ["left_frame", "right_frame"],
                  "engagement": 0.87
                }
              ],
              "sentiment": {
                "overall": "neutral_positive",
                "confidence": 0.82
              },
              "topics": ["technology", "artificial_intelligence"]
            }
          },
          "jsonDescription": "This JSON shows how content is classified, including categories, settings, subject information, and even sentiment analysis."
        }
      ],
      "analysisTitle": "Putting It All Together",
      "analysisContent": "By combining these elements, our system creates a comprehensive map of your video content. This structured data powers features like intelligent search, automated editing, and content analysis.",
      "fullExampleTitle": "Complete Scene Data Example",
      "fullExampleDescription": "Here's how all the pieces come together in a complete scene analysis:",
      "fullExample": {
        "scene_id": "scene_001",
        "start_time": 2.5,
        "end_time": 5.2,
        "duration": 2.7,
        "metadata": {
          "created_at": "2025-12-11T14:25:30Z",
          "video_source": "interview_001.mp4",
          "resolution": "1920x1080",
          "fps": 30
        },
        "visual_analysis": {
          "brightness": 0.78,
          "contrast": 0.65,
          "color_palette": ["#3A5FCD", "#87CEEB", "#F5F5DC"],
          "lighting_condition": "daylight",
          "environment": "studio"
        },
        "audio_analysis": {
          "has_speech": true,
          "speech_confidence": 0.92,
          "background_noise_level": 0.15,
          "speaker_gender": ["male", "female"],
          "speech_text": "Let's discuss how AI is transforming video production..."
        },
        "content_analysis": {
          "primary_category": "interview",
          "setting": "studio",
          "subjects": ["host", "guest"],
          "sentiment": "neutral_positive"
        },
        "shots": [
          {
            "shot_id": "shot_001",
            "start_time": 2.5,
            "end_time": 3.1,
            "keyframe": "https://example.com/keyframes/scene_001_shot_001.jpg",
            "transition": {"type": "cut", "confidence": 0.98}
          },
          {
            "shot_id": "shot_002",
            "start_time": 3.1,
            "end_time": 5.2,
            "keyframe": "https://example.com/keyframes/scene_001_shot_002.jpg",
            "transition": {"type": "fade", "confidence": 0.95}
          }
        ]
      },
      "benefitsTitle": "Key Benefits",
      "benefits": [
        "<strong>Efficient Editing:</strong> Jump directly to any scene or shot without scrubbing through hours of footage",
        "<strong>Smart Search:</strong> Find content based on visual elements, not just metadata",
        "<strong>Consistent Quality:</strong> Identify and maintain visual consistency across your project",
        "<strong>Data-Driven Decisions:</strong> Get insights into your content structure and pacing"
      ]
    },
    "conclusion": {
      "title": "Transforming Video Production with AI",
      "p1": "AI-powered scene detection is revolutionizing how we approach video production. By automating the tedious process of scene identification and organization, creators can focus on what truly matters ‚Äì telling compelling stories. Our technology bridges the gap between raw footage and polished content, making professional-grade video analysis accessible to everyone.",
      "p2": "As we continue to refine our algorithms and expand our capabilities, we're excited to see how filmmakers, educators, and content creators will leverage these tools to push the boundaries of visual storytelling. The future of video production is here, and it's more efficient and creative than ever before."
    }
  }
   


}