{
  "home": {
    "metadata": {
      "title": "Curify | AIë¡œ ì½˜í…ì¸  ì œì‘ ê°•í™”",
      "description": "CurifyëŠ” í¬ë¦¬ì—ì´í„°, êµìœ¡ì ë° ë¯¸ë””ì–´ íŒ€ì´ ë¹„ë””ì˜¤, ë§Œí™” ë° í”„ë ˆì  í…Œì´ì…˜ì„ ëŒ€ê·œëª¨ë¡œ ì œì‘í•˜ê³  í˜„ì§€í™”í•˜ë„ë¡ ë•ëŠ” AI ë„¤ì´í‹°ë¸Œ í”Œë«í¼ì…ë‹ˆë‹¤."
    },
    "hero": {
      "title": "AIë¡œ ì½˜í…ì¸  ì œì‘ ê°•í™”",
      "description": "Curify StudioëŠ” í¬ë¦¬ì—ì´í„°ì™€ ì¡°ì§ì´ ì–¸ì–´ ë° í˜•ì‹ ì¥ë²½ì„ ê·¹ë³µí•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” AI ë„¤ì´í‹°ë¸Œ ì½˜í…ì¸  ì œì‘ í”Œë«í¼ì„ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í†¤, ìŠ¤íƒ€ì¼ ë° ê°ì •ì  ê¹Šì´ë¥¼ ë³´ì¡´í•˜ëŠ” ì§„ì •í•œ ë²ˆì—­ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ê¸€ë¡œë²Œ ì²­ì¤‘ì—ê²Œ ì½˜í…ì¸ ë¥¼ í™•ì¥í•˜ëŠ” ê³¼ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ë¯¸ë””ì–´, êµìœ¡ ë° ì—”í„°í…Œì¸ë¨¼íŠ¸ì˜ êµì°¨ì ì—ì„œ ìš´ì˜í•˜ë©° ê¸‰ë³€í•˜ëŠ” ê¸€ë¡œë²Œ ì‚°ì—…ì—ì„œ í¬ë¦¬ì—ì´í„°ê°€ ì½˜í…ì¸ ë¥¼ ì›í™œí•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
    }
  },
  "coreFeatures": {
    "oneShot": {
      "title": "ì›ìƒ· ë²ˆì—­",
      "desc": "ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ìŒì„± í•´ì„¤, ìë§‰ ë° ë¦½ì‹±í¬ê°€ í¬í•¨ëœ ì „ì²´ ë¹„ë””ì˜¤ ë²ˆì—­."
    },
    "toneColor": {
      "title": "ìŒìƒ‰ ë³´ì¡´",
      "desc": "ì›ë˜ í™”ìì˜ ê³ ìœ í•œ ìŒì„± íŠ¹ì„±ê³¼ í†¤ í’ˆì§ˆì„ ìœ ì§€í•©ë‹ˆë‹¤."
    },
    "emotional": {
      "title": "ê°ì •ì  ë°œí™”",
      "desc": "AIëŠ” ê°ì •ì  ë‰˜ì•™ìŠ¤ë¥¼ ì¬í˜„í•˜ì—¬ ì–¸ì–´ ì „ë°˜ì— ê±¸ì³ ì§„ì •í•œ í‘œí˜„ì„ ë³´ì¥í•©ë‹ˆë‹¤."
    },
    "lipSync": {
      "title": "ë¦½ì‹±í¬ ê¸°ìˆ ",
      "desc": "ë²ˆì—­ëœ ì˜¤ë””ì˜¤ì™€ ì… ì›€ì§ì„ì„ ì™„ë²½í•˜ê²Œ ì¼ì¹˜ì‹œí‚¤ëŠ” ê³ ê¸‰ ë¦½ ë™ê¸°í™”."
    },
    "subtitle": {
      "title": "ìë§‰ ìº¡ì…”ë„ˆ",
      "desc": "ì •í™•í•œ íƒ€ì´ë°ê³¼ ìì—°ì–´ íë¦„ì„ ê°–ì¶˜ ì§€ëŠ¥í˜• ìë§‰ ìƒì„±."
    },
    "languages": {
      "title": "170ê°œ ì´ìƒì˜ ì–¸ì–´",
      "desc": "ë„¤ì´í‹°ë¸Œ ìˆ˜ì¤€ì˜ ì •í™•ë„ë¡œ 170ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ì½˜í…ì¸ ë¥¼ ë²ˆì—­í•˜ì„¸ìš”."
    }
  },
  "upcoming": {
    "title": "ê³§ ì¶œì‹œ ì˜ˆì •",
    "subtitle": "ê°œë°œ ì¤‘ì¸ ì°¨ì„¸ëŒ€ ê¸°ëŠ¥",
    "styleTransfer": {
      "title": "ìŠ¤íƒ€ì¼ ë³€í™˜",
      "desc": "AI ê¸°ë°˜ ì‹œê°ì  ë³€í™˜ì„ í†µí•´ ë¹„ë””ì˜¤ì— ì˜í™”ì  ë˜ëŠ” ë¸Œëœë“œë³„ ìŠ¤íƒ€ì¼ì„ ì ìš©í•˜ì„¸ìš”.",
      "icon": "ğŸ¨",
      "status": "2025ë…„ 3ë¶„ê¸° ì¶œì‹œ ì˜ˆì •",
      "transcript": "ì´ ë°ëª¨ëŠ” AI ê¸°ë°˜ ì‹œê°ì  ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‚¬ ë“œë¼ë§ˆ í´ë¦½ì„ ê¸°ë°œí•œ ì§€ë¸Œë¦¬ ìŠ¤íƒ€ì¼ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
    },
    "mangaTranslation": {
      "title": "ë§Œí™” ë²ˆì—­",
      "desc": "í…ìŠ¤íŠ¸ ê°ì§€, ë§í’ì„  í¸ì§‘ ë° ë¬¸í™”ì  ì ì‘ì„ í†µí•œ ìë™ ë§Œí™” ë° ë§Œí™” ë²ˆì—­.",
      "icon": "ğŸ“š",
      "status": "2025ë…„ 3ë¶„ê¸° ì¶œì‹œ ì˜ˆì •",
      "transcript": "ì´ í”„ë¡œí† íƒ€ì…ì€ ë§í’ì„  ê°ì§€ ë° ì´ì¤‘ ì–¸ì–´ ì¸í”Œë ˆì´ìŠ¤ í¸ì§‘ì„ í¬í•¨í•œ ìë™ ë§Œí™” ë²ˆì—­ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
    },
    "templatedVideo": {
      "title": "í…œí”Œë¦¿ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±",
      "desc": "AI ìƒì„± ì½˜í…ì¸  ë° ë§ì¶¤í˜• ë¸Œëœë”©ì´ í¬í•¨ëœ í…œí”Œë¦¿ìœ¼ë¡œ ì „ë¬¸ì ì¸ ë¹„ë””ì˜¤ë¥¼ ë§Œë“œì„¸ìš”.",
      "icon": "ğŸ¬",
      "status": "2025ë…„ 4ë¶„ê¸° ì¶œì‹œ ì˜ˆì •",
      "transcript": "ì´ ë°ëª¨ëŠ” ìŠ¤í† ë¦¬ë³´ë“œë¥¼ ê³„íší•˜ê³  ì‹œê°ì  ìš”ì†Œë¥¼ ì¡°ë¦½í•˜ì—¬ ì´ˆê¸° ë¯¸êµ­ ì—­ì‚¬ì— ëŒ€í•œ ì—­ì‚¬ì  ì¥ë©´ì„ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸-íˆ¬-ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
    },
    "statusQ3": "2025ë…„ 3ë¶„ê¸° ì¶œì‹œ ì˜ˆì •",
    "statusQ4": "2025ë…„ 4ë¶„ê¸° ì¶œì‹œ ì˜ˆì •"
  },
  "export": {
    "title": "ë‚´ë³´ë‚´ê¸°",
    "export": "ë‚´ë³´ë‚´ê¸°",
    "downloading": "ë‹¤ìš´ë¡œë“œ ì¤‘..."
  },
  "delete": {
    "title": "ì´ì¤‘ í™•ì¸",
    "message": "ì´ í”„ë¡œì íŠ¸ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
    "warning": "ì´ ì‘ì—…ì€ ì·¨ì†Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
    "cancel": "ì·¨ì†Œ",
    "delete": "ì‚­ì œ",
    "deleting": "ì‚­ì œ ì¤‘..."
  },
  "userMenu": {
    "topUpCredits": "í¬ë ˆë”§ ì¶©ì „",
    "remaining": "ë‚¨ìŒ",
    "planRemaining": "í”Œëœ ì”ì—¬",
    "validUntil": "ìœ íš¨ ê¸°ê°„",
    "creditsHistory": "í¬ë ˆë”§ ë‚´ì—­",
    "subscribePlan": "í”Œëœ êµ¬ë…",
    "supportTicket": "ì§€ì› í‹°ì¼“",
    "signOut": "ë¡œê·¸ì•„ì›ƒ"
  },
  "technology": {
    "multimodal": {
      "title": "ë©€í‹°ëª¨ë‹¬ ì¸ì‹",
      "desc": "ìš°ë¦¬ëŠ” ìŒì„± ë° ìë§‰ ì‹ í˜¸ë¥¼ ê²°í•©í•˜ì—¬ ë” ì •í™•í•˜ê³  ê°•ë ¥í•œ ì „ì‚¬ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ ì´ì¤‘ ì±„ë„ ì¸ì‹ì€ ì˜¤ë¥˜ë¥¼ ì¤„ì´ê³  íŠ¹íˆ ì‹œë„ëŸ½ê±°ë‚˜ ë³µì¡í•œ ì˜¤ë””ì˜¤ ì¥ë©´ì—ì„œ ì›ë³¸ ì½˜í…ì¸ ì™€ì˜ ë” ë‚˜ì€ ì •ë ¬ì„ ë³´ì¥í•©ë‹ˆë‹¤."
    },
    "emotional": {
      "title": "ê°ì •ì  ë°œí™”",
      "desc": "ìš°ë¦¬ì˜ ìŒì„± í•©ì„± ëª¨ë¸ì€ ìŠ¤í† ë¦¬í…”ë§ê³¼ ì‹œì²­ì ì°¸ì—¬ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” í‘œí˜„ë ¥ì´ í’ë¶€í•˜ê³  ê°ì •ì ìœ¼ë¡œ í’ë¶€í•œ ì—°ì„¤ì„ ìƒì„±í•©ë‹ˆë‹¤. í†¤, ë¦¬ë“¬ ë° ë‰˜ì•™ìŠ¤ë¥¼ í¬ì°©í•˜ì—¬ AI ìŒì„±ì„ ë” ì¸ê°„ì ì´ê³  ê³µê°í•  ìˆ˜ ìˆê²Œ ë§Œë“­ë‹ˆë‹¤."
    },
    "lengthaware": {
      "title": "ê¸¸ì´ ì¸ì‹ ë²ˆì—­ ë° ì‚¬ìš©ì ì •ì˜",
      "desc": "ìš°ë¦¬ëŠ” ì •í™•ì„±ë¿ë§Œ ì•„ë‹ˆë¼ ë¹„ë””ì˜¤ ë° ìŒì„± ì •ë ¬ì— ì¤‘ìš”í•œ íƒ€ì´ë°ê³¼ í˜ì´ì‹±ì„ ìœ„í•´ì„œë„ ë²ˆì—­ì„ ìµœì í™”í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ë‹¤ì–‘í•œ ì½˜í…ì¸  ìš”êµ¬ ì‚¬í•­ì´ë‚˜ ì²­ì¤‘ ì„ í˜¸ë„ì— ë§ê²Œ í†¤, ê¸¸ì´ ë° ë¬¸êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©ì ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    },
    "controlled": {
      "title": "ì œì–´ëœ ë¹„ë””ì˜¤ ìƒì„±",
      "desc": "ìš°ë¦¬ëŠ” ì œì–´ ê°€ëŠ¥í•œ ì‹œê°ì  ìš”ì†Œ ë° ì „í™˜ì„ í†µí•´ êµ¬ì¡°í™”ëœ í…œí”Œë¦¿ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” í¬ë¦¬ì—ì´í„°ì—ê²Œ ì°½ì˜ì ì¸ ììœ ì™€ ì œì‘ ì¼ê´€ì„±ì„ ëª¨ë‘ ì œê³µí•˜ì—¬ ìˆ˜ë™ ì‘ì—…ì„ ì¤„ì´ëŠ” ë™ì‹œì— ê³ í’ˆì§ˆ ì¶œë ¥ì„ ë³´ì¥í•©ë‹ˆë‹¤."
    }
  },
  "tools": {
    "video_dubbing": {
      "title": "ë¹„ë””ì˜¤ ë”ë¹™",
      "desc": "ì •í™•í•œ í˜„ì§€í™” ë° ìŒì„± ë™ê¸°í™”ë¥¼ í†µí•´ ë¹„ë””ì˜¤ë¥¼ ëª¨ë“  ì–¸ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”"
    },
    "subtitle_captioner": {
      "title": "ìë§‰ ìº¡ì…”ë„ˆ",
      "desc": "ëª…í™•ì„±ê³¼ ì ‘ê·¼ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤êµ­ì–´ ìë§‰ ìë™ ìƒì„±"
    },
    "lip_syncing": {
      "title": "ë¦½ì‹±í¬",
      "desc": "AI ê¸°ë°˜ ë¦½ì‹±í¬ë¡œ ì…ìˆ ì„ ë§ê³¼ ì™„ë²½í•˜ê²Œ ì¼ì¹˜ì‹œí‚¤ì„¸ìš”"
    },
    "style_transfer": {
      "title": "ìŠ¤íƒ€ì¼ ë³€í™˜",
      "desc": "ë¹„ë””ì˜¤ë¥¼ Pixar, Ghibli ë˜ëŠ” ê¸°íƒ€ ì˜ˆìˆ ì  ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•˜ì„¸ìš” â€” ê³§ ì¶œì‹œ ì˜ˆì •"
    },
    "coming_soon": "ê³§ ì¶œì‹œ ì˜ˆì •",
    "create": "ë§Œë“¤ê¸°"
  },
  "bilingual": {
    "metadata": {
      "title": "ë¬´ë£Œ ì´ì¤‘ ì–¸ì–´ ìë§‰ ìƒì„±ê¸° | Curify AI",
      "description": "Curifyì˜ ë¬´ë£Œ AI ê¸°ë°˜ ë„êµ¬ë¡œ ë¹„ë””ì˜¤ì— ëŒ€í•œ ì´ì¤‘ ì–¸ì–´ ìë§‰ì„ ë§Œë“œì„¸ìš”. YouTube, TikTok, êµìœ¡ ë° ê¸€ë¡œë²Œ í¬ë¦¬ì—ì´í„°ì—ê²Œ ì í•©í•©ë‹ˆë‹¤."
    },
    "title": "ì‰¬ìš´ ì´ì¤‘ ì–¸ì–´ ìë§‰",
    "intro": "Curifyì˜ AI ìë§‰ ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ ì–¸ì–´ë¡œ ìë§‰ì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”. í¬ë¦¬ì—ì´í„°, êµìœ¡ì ë° ê¸€ë¡œë²Œ ê¸°ì—…ì— ì´ìƒì ì…ë‹ˆë‹¤.",
    "example": "ì˜ˆ: AI ì „ëµì„ ì„¤ëª…í•˜ëŠ” ì  ìŠ¨ í™© â€” ì˜ì–´ + ì¤‘êµ­ì–´ ìë§‰ ìë™ ìƒì„±.",
    "cta": "ë¬´ë£Œë¡œ ì‚¬ìš©í•´ ë³´ì„¸ìš”",
    "why": {
      "title": "ì´ì¤‘ ì–¸ì–´ ìë§‰ì— Curifyë¥¼ ì„ íƒí•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?",
      "point1": "ì •í™•í•œ ë²ˆì—­ìœ¼ë¡œ 170ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.",
      "point2": "ê°ì •ì  í†¤ê³¼ íƒ€ì´ë°ì„ ë³´ì¡´í•©ë‹ˆë‹¤.",
      "point3": "YouTube, TikTok ë° êµìœ¡ìš© ë¹„ë””ì˜¤ì— ì í•©í•©ë‹ˆë‹¤.",
      "point4": "ê²Œì‹œ ì¤€ë¹„ê°€ ëœ ìë§‰ íŒŒì¼ ë˜ëŠ” ì„ë² ë””ë“œ ë¹„ë””ì˜¤ë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤."
    },
    "faq": {
      "title": "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸",
      "q1": "ì •ë§ ë¬´ë£Œì¸ê°€ìš”?",
      "a1": "ì˜ˆ, CurifyëŠ” ë§¤ì›” ì œí•œëœ ìë§‰ ì‹œê°„ì„ ì œê³µí•˜ëŠ” ë¬´ë£Œ í”Œëœì„ ì œê³µí•©ë‹ˆë‹¤.",
      "q2": "ìƒì„± í›„ ìë§‰ì„ í¸ì§‘í•  ìˆ˜ ìˆë‚˜ìš”?",
      "a2": "ë¬¼ë¡ ì…ë‹ˆë‹¤. ë‚´ë³´ë‚´ê¸° ì „ì— íƒ€ì´ë°, ë²ˆì—­ ë° ìŠ¤íƒ€ì¼ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    }
  },
  "videoDubbing": {
    "metadata": {
      "title": "AI ë¹„ë””ì˜¤ ë”ë¹™ ë„êµ¬ | Curify AI",
      "description": "AI ìŒì„± ë³µì œ, ê°ì • ë³´ì¡´ ë° ë¦½ì‹±í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ë¥¼ 170ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ë”ë¹™í•˜ì„¸ìš”. Curifyì˜ AI ë”ë¹™ì„ ë¬´ë£Œë¡œ ì‚¬ìš©í•´ ë³´ì„¸ìš”."
    },
    "title": "ìì—°ìŠ¤ëŸ¬ìš´ ëª©ì†Œë¦¬ì˜ AI ë¹„ë””ì˜¤ ë”ë¹™",
    "description": "í˜„ì‹¤ì ì¸ AI ìŒì„± ë° ë¦½ì‹±í¬ë¡œ ë¹„ë””ì˜¤ ì½˜í…ì¸ ë¥¼ 170ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê³  ë”ë¹™í•˜ì„¸ìš”. êµ­ì œ í¬ë¦¬ì—ì´í„° ë° ë¸Œëœë“œì— ì í•©í•©ë‹ˆë‹¤."
  },
  "about": {
    "metadata": {
      "title": "Curify ì†Œê°œ | ë¹„ì „, ê¸°ìˆ  ë° íŒ€",
      "description": "AIë¡œ ì½˜í…ì¸  ì œì‘ì„ ë¯¼ì£¼í™”í•˜ë ¤ëŠ” Curifyì˜ ë¹„ì „ì— ëŒ€í•´ ì•Œì•„ë³´ì„¸ìš”. ìš°ë¦¬ëŠ” ë¯¸ë””ì–´ í˜„ì§€í™”ì˜ ë¯¸ë˜ë¥¼ êµ¬ì¶•í•˜ëŠ” ì—”ì§€ë‹ˆì–´ ë° í¬ë¦¬ì—ì´í„° íŒ€ì…ë‹ˆë‹¤."
    }
  },
  "contact": {
    "metadata": {
      "title": "ë¬¸ì˜í•˜ê¸° | Curify Studio",
      "description": "ì§€ì›, íŒë§¤ ë˜ëŠ” íŒŒíŠ¸ë„ˆì‹­ì„ ìœ„í•´ Curify íŒ€ì— ë¬¸ì˜í•˜ì„¸ìš”. ìš°ë¦¬ëŠ” ê·€í•˜ê°€ ì½˜í…ì¸ ë¥¼ ì „ ì„¸ê³„ì ìœ¼ë¡œ í™•ì¥í•˜ë„ë¡ ë•ê¸° ìœ„í•´ ì—¬ê¸° ìˆìŠµë‹ˆë‹¤."
    }
  },
  "blog": {
    "metadata": {
      "title": "Curify ë¸”ë¡œê·¸ | AI ì½˜í…ì¸  ì œì‘ ì¸ì‚¬ì´íŠ¸",
      "description": "AI ë¹„ë””ì˜¤ ìƒì„±, ë”ë¹™, ìŠ¤íƒ€ì¼ ë³€í™˜ ë° ì½˜í…ì¸  ì œì‘ì˜ ë¯¸ë˜ì— ëŒ€í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ì½ì–´ë³´ì„¸ìš”."
    }
  },
  "pricing": {
    "metadata": {
      "title": "ê°€ê²© í”Œëœ | Curify Studio",
      "description": "ì½˜í…ì¸  ì œì‘ ìš”êµ¬ ì‚¬í•­ì— ë§ëŠ” í”Œëœì„ ì„ íƒí•˜ì„¸ìš”. ë¬´ë£Œ ë„êµ¬ì—ì„œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ë”ë¹™ ë° í˜„ì§€í™”ê¹Œì§€."
    },
    "header": {
      "title": "í”Œëœìœ¼ë¡œ ì‹œì‘í•˜ê¸°",
      "subtitle": "í•„ìš”ì— ë§ëŠ” í”Œëœì„ ì„ íƒí•˜ì„¸ìš”."
    },
    "common": {
      "month": "ì›”",
      "pricing": "ê°€ê²©",
      "receive": "ìˆ˜ì‹ "
    },
    "buttons": {
      "signUp": "ê°€ì…í•˜ê¸°",
      "subscribePlan": "í”Œëœ êµ¬ë…",
      "currentPlan": "í˜„ì¬ í”Œëœ",
      "comingSoon": "ê³§ ì¶œì‹œ ì˜ˆì •",
      "downgradeToFree": "ë¬´ë£Œë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ",
      "downgradeToCreator": "Creatorë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ",
      "contactSales": "ì˜ì—…íŒ€ ë¬¸ì˜"
    },
    "plans": {
      "free": {
        "name": "ë¬´ë£Œ",
        "description": "ë„êµ¬ í…ŒìŠ¤íŠ¸ ë° ê°€ë²¼ìš´ ì‘ì—…ì— ì í•©",
        "features": [
          "ì›Œí„°ë§ˆí¬ ì—†ì´ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ",
          "1ì‹œê°„ ë¬´ë£Œ ìë§‰ ì²˜ë¦¬",
          "SRT íŒŒì¼ ë‚´ë³´ë‚´ê¸°"
        ]
      },
      "creator": {
        "name": "Creator",
        "description": "ì •ê¸°ì ì¸ ìë§‰ ë° ê°€ë²¼ìš´ ë”ë¹™ì„ í•˜ëŠ” ì†Œê·œëª¨ í¬ë¦¬ì—ì´í„°ì—ê²Œ ì´ìƒì ",
        "plusTitle": "ë¬´ë£Œì˜ ëª¨ë“  ê²ƒ, í”ŒëŸ¬ìŠ¤:",
        "features": [
          "ë¦½ì‹±í¬",
          "ì¼ê´„ ì²˜ë¦¬",
          "5ì‹œê°„ ë¬´ë£Œ ìë§‰ ì²˜ë¦¬",
          "ì‘ì—…ë‹¹ ìµœëŒ€ 30ë¶„"
        ]
      },
      "pro": {
        "name": "Pro",
        "description": "ê³§ ì¶œì‹œ ì˜ˆì • â€” ëŒ€ëŸ‰ í¬ë¦¬ì—ì´í„°ë¥¼ ìœ„í•´ ì„¤ê³„ë¨",
        "plusTitle": "Creatorì˜ ëª¨ë“  ê²ƒ, í”ŒëŸ¬ìŠ¤:",
        "features": [
          "ìŒì„± ë¯¸í™” ë° ë…¸ì´ì¦ˆ ì œê±°",
          "ìš°ì„  ìˆœìœ„ ëŒ€ê¸°ì—´",
          "ì‘ì—…ë‹¹ ìµœëŒ€ 60ë¶„",
          "ë¬´ì œí•œ ìë§‰ ì‹œê°„"
        ]
      },
      "enterprise": {
        "name": "Enterprise",
        "description": "íŒ€, ìŠ¤íŠœë””ì˜¤ ë° ì—”í„°í”„ë¼ì´ì¦ˆ ì›Œí¬í”Œë¡œìš©",
        "customPricing": "ë§ì¶¤í˜•",
        "unlimited": "ë¬´ì œí•œ ğŸš",
        "tailoredSupport": "ë° ë§ì¶¤í˜• ì§€ì›",
        "plusTitle": "Proì˜ ëª¨ë“  ê²ƒ, í”ŒëŸ¬ìŠ¤:",
        "features": [
          "ì „ë‹´ ê³„ì • ê´€ë¦¬ì",
          "ì˜¨í”„ë ˆë¯¸ìŠ¤ ë°°í¬ ì˜µì…˜",
          "API ì•¡ì„¸ìŠ¤ ë° ì‚¬ìš© ë¶„ì„",
          "ë§ì¶¤í˜• í†µí•© ë° SLA",
          "ë¬´ì œí•œ ì²˜ë¦¬ ì‹œê°„"
        ]
      }
    },
    "table": {
      "header": {
        "feature": "ê¸°ëŠ¥ / ì œí•œ"
      },
      "rows": {
        "videoDownloadWithWatermark": "ì›Œí„°ë§ˆí¬ í¬í•¨ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ",
        "videoDownloadWithoutWatermark": "ì›Œí„°ë§ˆí¬ ì—†ì´ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ",
        "downloadSrt": "SRT íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        "voiceBeautification": "ìŒì„± ë¯¸í™” ë° ë…¸ì´ì¦ˆ ì œê±°",
        "lipSync": "ë¦½ì‹±í¬",
        "subtitleTools": "ìë§‰ ë„êµ¬(ì¶”ê°€/ì œê±°, ë²ˆì—­ ì—†ìŒ)",
        "batchProcessing": "ì¼ê´„ ì²˜ë¦¬",
        "maxVideoLength": "ì‘ì—…ë‹¹ ìµœëŒ€ ë¹„ë””ì˜¤ ê¸¸ì´",
        "monthlyCredits": "ì›”ê°„ ë¬´ë£Œ í¬ë ˆë”§",
        "priorityQueue": "ìš°ì„  ìˆœìœ„ ëŒ€ê¸°ì—´ / ë” ë¹ ë¥¸ ì²˜ë¦¬",
        "creditTopUp": "í¬ë ˆë”§ ì¶©ì „"
      },
      "values": {
        "free": "ë¬´ë£Œ",
        "unlimited": "ë¬´ì œí•œ",
        "oneHourPerMonth": "1ì‹œê°„/ì›”",
        "fiveHoursPerMonth": "5ì‹œê°„/ì›”",
        "fiveMinutes": "5ë¶„",
        "thirtyMinutes": "30ë¶„",
        "sixtyMinutes": "60ë¶„"
      }
    }
  },
  "aeVsComfyUi": {
    "title": "AE vs ComfyUI â€“ Redefining Animation (Part 3)",
    "heading": "ğŸ¨ AE vs ComfyUI",
    "intro1": "We often get asked: why not just use After Effects (AE)? Or why bother with ComfyUI? Both are essential â€” one provides fine control, the other generative creativity.",
    "intro2": "AE offers timeline precision and control over transitions, lighting, and effects. ComfyUI brings AI-native workflows, combining diffusion, text, and multimodal orchestration.",
    "table": {
      "colAE": "AE",
      "colComfy": "ComfyUI",
      "row1a": "Precise VFX and motion control",
      "row1b": "AI-native generation of videos",
      "row2a": "Timeline-based editing",
      "row2b": "Workflow/node-based rendering",
      "row3a": "Manual / plugin-driven workflow",
      "row3b": "Diffusion + LLM + multimodal orchestration"
    },
    "outro": "In our workflow, ComfyUI handles the AI generation and iteration, while AE refines the final look â€” transitions, overlays, and compositing."
  },
  "video_translation_eval": {
    "title": "Evaluating AI Video Translation Quality",
    "subtitle": "Metrics that Matter",
    "intro": "Translating videos across languages is no small feat â€” it involves transcription, translation, voice synthesis, timing, and more. At Curify, weâ€™ve built a robust evaluation pipeline to ensure each piece meets industry standards.",
    "section1_title": "1. Transcription Quality",
    "section1_engine": "Engine: WhisperX",
    "section1_metrics": [
      "WER (Word Error Rate)",
      "Punctuation F1 (for expressiveness and readability)"
    ],
    "section2_title": "2. Translation Quality",
    "section2_engine": "Engines: Helsinki, MarianMT",
    "section2_metrics": [
      "BLEU (standard metric)",
      "COMET / chrF++ (semantic similarity)",
      "Human review: fluency + adequacy"
    ],
    "section3_title": "3. Voice Synthesis Quality",
    "section3_engine": "Engines: XTTS / YourTTS",
    "section3_metrics": [
      "MOS (Naturalness, similarity, expressiveness)",
      "Speaker verification accuracy"
    ],
    "section4_title": "4. Alignment & Lip Sync",
    "section4_metrics": [
      "Segment duration mismatch",
      "Wav2Lip sync confidence",
      "Temporal drift analysis"
    ],
    "section5_title": "5. Semantic Preservation",
    "section5_method": "We use LLMs (like GPT-4) to judge whether the translated speech preserves the original meaning, tone, and emotion. Example prompt:",
    "section5_prompt": "Compare this Mandarin transcript to the English voiceover. Does the tone, intent, and content match? Rate 1â€“5 and explain.",
    "section6_title": "6. User Feedback & GTM Validation",
    "section6_metrics": [
      "Voice quality fit for product category",
      "Viewer retention improvement",
      "Adoption willingness from early users (e.g., 1688 sellers)"
    ],
    "closing": "At Curify, we donâ€™t just translate â€” we preserve storytelling, emotion, and clarity across languages. If youâ€™re ready to scale your voice globally, get in touch.",
    "available_locales": "Post available in: [EN] [ZH] [ES] [DE]"
  },
  "storyboardToPipeline": {
    "title": "From Storyboards to AI Pipelines â€“ Redefining Animation",
    "intro": {
      "p1": "Most people think AI video means \"text in, clip out.\" But if you're aiming for <strong>cinematic, director-level control</strong>, it's an entirely different game.",
      "p2": "In traditional animation, every detail matters â€” character design, motion continuity, timing, and scene transitions. Our goal is to make AI match that level of precision.",
      "p3": "Animation today is both an art and a structured orchestration challenge. We think like directors, but build like engineers.",
      "p4": "That's why we build <strong>Controlled Generation Pipelines</strong> instead of one-shot generation. These pipelines combine structure and creativity:",
      "pipelineItems": [
        "Storyboard-to-Video: JSON-based scene specs with IDs, timing, and camera metadata",
        "ComfyUI-based workflows: node-graph DAGs with versioned checkpoints, LoRA stacks, and seeds",
        "Temporal & multimodal control: shared scene IDs across image, motion, audio, and subtitles"
      ],
      "p5": "Now, let's walk through a simple example to show how AI pipelines work in practice.",
      "technicalNotes": [
        "All stages read/write typed artifacts (JSON/CSV, PNG sequences, WAV/MP3, MP4, .aep)",
        "Pipelines are orchestrated via Python/CLI (e.g., ComfyUI API, ffmpeg, TTS APIs)",
        "Every run is reproducible through stored config (model, sampler, seed, prompts, duration)"
      ]
    },
    "pipeline": {
      "title": "AI Video Generation Pipeline",
      "description": "The AI video generation pipeline transforms text prompts into polished videos through structured stages with explicit inputs, outputs, and configs.",
      "prompt": "1. Prompt (raw idea â†’ structured JSON spec)",
      "storyboard": "2. Storyboard (scene/shot table with timing, camera, and description)",
      "images": "3. Images (per-shot keyframes generated via Stable Diffusion / ComfyUI)",
      "animation": "4. Animation (image sequences â†’ motion, parallax, and effects)",
      "voiceOver": "5. Voice Over (TTS + alignment data)",
      "finalVideo": "6. Final Video (ffmpeg composition: video + audio + subtitles)",
      "features": [
        "JSON-first design: every scene is addressable and scriptable (scene_id, shot_id)",
        "ComfyUI-based workflows: modular, reproducible, composable DAGs for image/video generation",
        "Temporal & multimodal control: consistent seeds, character embeddings, and timing across modalities"
      ],
      "exampleIntro": "Now, let's walk through a simple example to show how AI pipelines work in practice.",
      "artifacts": {
        "promptSpec": "prompt_run_001.json",
        "storyboardTable": "storyboard_v1.csv",
        "imageOutputDir": "renders/scene_{scene_id}/shot_{shot_id}/frame_####.png",
        "audioOutput": "audio/voiceover_final.wav",
        "subtitleFile": "subtitles/storyboard_v1.srt",
        "finalVideoFile": "exports/storyboard_v1_final.mp4"
      }
    },
    "steps": {
      "step1": {
        "title": "Step 1: Start with a Basic Prompt",
        "example": "A girl stands at a midnight train station, wind blowing her hair.",
        "expandedPrompt": "With the help of GPT or a local LLM, we expand this into a structured JSON object with global style, character definitions, and per-scene breakdown.",
        "technicalDetails": {
          "llmFields": [
            "global_style: art style, camera language, color palette",
            "characters: name, age, look, outfit, emotional state",
            "scenes: [{ scene_id, shot_id, description, camera_type, duration_seconds }]"
          ],
          "outputFile": "prompt_run_001.json",
          "exampleStack": "Python script calling OpenAI/LLM API and writing JSON to disk"
        }
      },
      "step2": {
        "title": "Step 2: Convert Prompt to a Storyboard Table",
        "description": "The JSON spec is flattened into a storyboard table (CSV or Notion) that both humans and the pipeline can read.",
        "technicalDetails": {
          "columns": [
            "scene_id",
            "shot_id",
            "start_time",
            "end_time",
            "duration_seconds",
            "camera_type",
            "visual_description",
            "character_state",
            "motion_notes"
          ],
          "storage": "storyboard_v1.csv (used as the single source of truth for downstream steps)",
          "automation": "Python / Node.js script transforms prompt_run_001.json â†’ storyboard_v1.csv"
        }
      },
      "step3": {
        "title": "ğŸ› ï¸ Step 3: Generate Visuals",
        "description": "Generate high-quality keyframe images for each shot using Stable Diffusion through a ComfyUI workflow.",
        "points": [
          "ğŸ¨ Use <strong>Stable Diffusion</strong> or <strong>ComfyUI</strong> to turn each row in `storyboard_v1.csv` into a high-res keyframe.",
          "Keep the style consistent by using the same base checkpoint, LoRA stack, sampler, and seed policy across all shots.",
          "Refine images with inpainting (for faces/hands) and outpainting (for extended compositions and camera motion)."
        ],
        "technicalDetails": {
          "modelConfig": {
            "checkpoint": "sd_xl_base_1.0.safetensors",
            "vae": "sdxl_vae.safetensors",
            "sampler": "euler_a",
            "steps": 25,
            "cfgScale": 6.5,
            "resolution": "1024x576",
            "loraStack": [
              "character_girl_v2.safetensors",
              "style_cinematic_blue_hour.safetensors"
            ]
          },
          "consistency": {
            "seedStrategy": "fixed_per_scene",
            "seedExample": "seed = hash(scene_id + character_name)",
            "namingConvention": "renders/scene_{scene_id}/shot_{shot_id}/kf_0001.png"
          },
          "comfyui": {
            "workflowFile": "workflows/storyboard_to_frame.json",
            "inputMethod": "custom node / HTTP API reading storyboard_v1.csv and modelConfig",
            "batching": "GPU batches keyed by scene_id to reuse loaded weights"
          }
        }
      },
      "step4": {
        "title": "ğŸ¬ Step 4: Add Motion and Atmosphere in After Effects",
        "description": "Enhance static keyframes with motion, parallax, and atmosphere using Adobe After Effects (or an equivalent compositor).",
        "points": [
          "Import image sequences or keyframes into <strong>Adobe After Effects</strong> as layered compositions.",
          "Apply keyframe animations: pan, zoom, parallax layers, fog overlays, glow and light flicker.",
          "Add ambient sound cues and cinematic transitions between scenes."
        ],
        "technicalDetails": {
          "input": "renders/scene_{scene_id}/shot_{shot_id}/kf_0001.png (plus optional depth/alpha maps)",
          "projectFile": "ae/storyboard_v1.aep",
          "automation": [
            "Use AE scripts/ExtendScript or JSX to auto-create comps per scene_id.",
            "Map `duration_seconds` from storyboard_v1.csv â†’ comp duration.",
            "Generate intermediate frames via AE plugins or video-aware diffusion tools if needed."
          ],
          "output": "ae_renders/scene_{scene_id}_shot_{shot_id}_anim.mp4"
        }
      },
      "step5": {
        "title": "ğŸ§ Step 5: Add Voice and Subtitles",
        "description": "Generate voiceover aligned to the storyboard and attach subtitles for accessibility and clarity.",
        "points": [
          "Use <strong>XTTS</strong> or <strong>ElevenLabs</strong> to generate natural voiceovers from the script, using a consistent speaker profile.",
          "For acronyms (like API, NBA), generate English snippets separately and merge in post to keep pronunciation clean.",
          "Add subtitles using `.srt` or `.json` timeline files synced to the voiceover track."
        ],
        "technicalDetails": {
          "scriptSource": "dialogue and narration pulled from prompt_run_001.json + storyboard_v1.csv",
          "ttsConfig": {
            "engine": "XTTS or ElevenLabs",
            "voiceId": "cinematic_female_en",
            "sampleRate": 44100,
            "format": "wav"
          },
          "outputs": {
            "voiceFile": "audio/storyboard_v1_voiceover.wav",
            "subtitleFile": "subtitles/storyboard_v1.srt"
          },
          "syncStrategy": "align `start_time` / `end_time` from storyboard_v1.csv with subtitle cues; ensure total audio length â‰ˆ final timeline duration"
        }
      },
      "step6": {
        "title": "Step 6: Final Composition with FFMPEG",
        "description": "Use FFMPEG to combine all pieces into one final video file with audio and subtitles.",
        "technicalDetails": {
          "inputs": {
            "video": "ae_renders/storyboard_v1_timeline.mp4",
            "audio": "audio/storyboard_v1_voiceover.wav",
            "subtitles": "subtitles/storyboard_v1.srt"
          },
          "ffmpegExample": "ffmpeg -i storyboard_v1_timeline.mp4 -i storyboard_v1_voiceover.wav -c:v libx264 -c:a aac -shortest -vf subtitles=subtitles/storyboard_v1.srt exports/storyboard_v1_final.mp4",
          "output": "exports/storyboard_v1_final.mp4 (H.264, 24 fps, AAC audio)"
        }
      }
    },
    "whatYouNeed": "ğŸ“ What You'll Need",
    "whatYouNeedTechnical": [
      "GPU with at least 12GB VRAM for Stable Diffusion / ComfyUI",
      "Python environment for JSON/CSV transforms and automation scripts",
      "ComfyUI installed with required custom nodes and workflows",
      "Adobe After Effects or similar compositor for motion and FX",
      "XTTS / ElevenLabs API access for TTS",
      "ffmpeg installed and available on the command line"
    ],
    "cta": "ğŸš€ Ready to bring your storyboard to life with AI? <strong>We can provide a full starter kit</strong> with example JSONs, ComfyUI workflows, and ffmpeg/AE templates to help you get started."
  },
  "SceneDetection": {
    "tags": {
      "computerVision": "Computer Vision",
      "deepLearning": "Deep Learning",
      "realTimeAnalysis": "Real-time Analysis"
    },
    "hero": {
      "title": "Transform Video into Storyboards with AI",
      "subtitle": "How we built an advanced pipeline that turns hours of footage into structured, searchable storyboards in minutes."
    },
    "introduction": {
      "paragraph1": "Imagine being able to upload hours of raw footage and within minutes get a <strong>detailed, scene-by-scene breakdown</strong> of your entire video. That's exactly what our AI-powered scene detection system delivers.",
      "paragraph2": "Built with cutting-edge Python libraries and deep learning models, this pipeline doesn't just detect scene changesâ€”it understands the content, identifies key elements, and structures everything into a comprehensive storyboard."
    },
    "imageAlt": "The scene detection pipeline in action, identifying key moments and generating structured storyboards",
    "protip": "For optimal results, ensure your video has clear visual separation between scenes. The system works best with well-lit footage and minimal motion blur. Consider adding chapter markers or scene breaks in your video editor to improve detection accuracy.",
    "humanEvaluation": {
      "title": "Human Evaluation of AI Output",
      "introduction": "While our AI pipeline is highly accurate, human evaluation remains a critical component of our quality assurance process. Here's how we ensure the highest quality output:",
      "evaluationAreas": [
        "<strong>Scene Boundary Accuracy</strong> - Reviewing and adjusting scene transition points",
        "<strong>Metadata Validation</strong> - Verifying AI-generated descriptions, tags, and labels",
        "<strong>Consistency Check</strong> - Ensuring uniform style and formatting across all scenes",
        "<strong>Content Accuracy</strong> - Validating that the AI correctly identified key elements in each scene"
      ],
      "workflow": {
        "title": "Human-in-the-Loop Workflow",
        "steps": [
          "AI processes the video and generates initial storyboard",
          "Human reviewers evaluate the AI output using our specialized tools",
          "Reviewers provide feedback and make necessary adjustments",
          "System learns from human corrections to improve future outputs",
          "Final quality check before delivery"
        ]
      },
      "benefits": {
        "title": "Benefits of Human Evaluation",
        "items": [
          "<strong>Higher Accuracy</strong> - Human reviewers catch subtle nuances that AI might miss",
          "<strong>Contextual Understanding</strong> - Better interpretation of cultural and situational context",
          "<strong>Quality Assurance</strong> - Additional layer of validation for professional use cases",
          "<strong>Continuous Improvement</strong> - Human feedback helps train and refine the AI models"
        ]
      },
      "tools": {
        "title": "Evaluation Tools",
        "description": "Our platform includes specialized tools to assist human reviewers:",
        "features": [
          "Side-by-side video and storyboard comparison",
          "Easy-to-use interface for adjusting scene boundaries",
          "Bulk editing of metadata and tags",
          "Collaboration features for team review",
          "Version history and change tracking"
        ]
      }
    },
    "header": {
      "title": "From Raw Footage to Storyboards",
      "author": "Curify AI Team",
      "role": "AI Research Team",
      "imageAlt": "AI analyzing video scenes and generating storyboards",
      "imageCaption": "AI-powered scene detection in action, identifying key moments in video content",
      "subtitle": "AI-Powered Video Analysis",
      "date": "December 11, 2025",
      "readingTime": "8 min read"
    },
    "Technical": {
      "title": "TECHNICAL DEEP DIVE",
      "steps": {
        "step1": "How It Works: Under the Hood",
        "videoProcessingPipeline": "Our system processes videos through a sophisticated multi-stage pipeline that ensures accurate scene detection and analysis:",
        "step2": "Video Processing Pipeline",
        "step2Text": "Our system processes videos through a sophisticated multi-stage pipeline that ensures accurate scene detection and analysis:",
        "step3": "Scene Detection and Metadata Extraction",
        "step3Text": "OpenCV-powered frame sampling at optimal intervals",
        "step4": "Shot Analysis and Metadata Enhancement",
        "step4Text": "Adaptive thresholding for precise scene boundary detection",
        "step5": "Scene Labeling and Metadata Organization",
        "step5Text": "Deep learning models for comprehensive scene understanding",
        "step6": "Metadata Export and Integration",
        "step6Text": "Export metadata to JSON format for integration with other tools"
      }
    },
    "featuresTitle": "Powerful Features at Your Fingertips",
    "features1": "Seamless Video Integration",
    "features1Text": "Process local files, YouTube links, or cloud storage with our unified interface.",
    "features": {
      "modularArchitecture": {
        "title": "Modular Architecture",
        "description": "The system is built with separate components for video analysis, AI processing, and output generation, making it easy to extend and maintain."
      },
      "performance": {
        "title": "Performance Optimized",
        "description": "Efficient frame processing and parallelization ensure fast analysis even for long videos."
      },
      "aiEnhanced": {
        "title": "AI-Enhanced Analysis",
        "description": "Optional AI components provide deeper scene understanding and more accurate labeling."
      }
    },
    "features2": "Seamless Video Integration",
    "features2Text": "Process local files, YouTube links, or cloud storage with our unified interface.",
    "features3": "AI-Powered Analysis",
    "features3Text": "Enhance scene understanding with our optional AI analysis module.",
    "features4": "Camera Motion Detection",
    "features4Text": "Automatically identify pans, zooms, and other camera movements.",
    "features5": "Customizable Output",
    "features5Text": "Export metadata to JSON format for integration with other tools.",
    "intro": {
      "p1": "In today's fast-paced content creation landscape, <strong>efficient video analysis</strong> is no longer a luxuryâ€”it's a necessity. Our AI-powered scene detection technology transforms hours of raw footage into structured, searchable content in minutes, not days.",
      "p2": "Whether you're a filmmaker, content creator, or media professional, understanding the power of automated scene detection can revolutionize your workflow and unlock new creative possibilities."
    },
    "proTip": "For best results, ensure your source video has clear audio and visual separation between scenes. Well-lit environments and minimal background noise significantly improve detection accuracy.",
    "titanicExample": {
      "title": "Real-World Example: Titanic Scene Analysis",
      "description": "Watch how our system analyzes a scene from Titanic, detecting shot changes and generating detailed scene metadata:",
      "analysis": "Analysis:",
      "analysisText": "Scene detection and metadata extraction in real-time"
    },
    "inceptionExample": {
      "title": "Dream Level Analysis: Inception Scene Breakdown",
      "description": "Explore how our AI analyzes the complex dream layers and visual effects in Inception:",
      "analysis": "Analysis",
      "analysisText": "Dream layer detection and visual effect breakdown",
      "breakdownTitle": "Scene Analysis Breakdown",
      "showFullAnalysis": "Show Full Scene Analysis",
      "hideFullAnalysis": "Hide Full Scene Analysis",
      "fullSceneAnalysis": "Full Scene Analysis",
      "sceneCard": {
        "mood": "Mood",
        "environment": "Environment",
        "shotNotes": "Shot Notes"
      }
    },
    "Scene": "Scene",
    "process": {
      "title": "How Our Scene Detection Works",
      "steps": {
        "step1": {
          "title": "1. Video Analysis",
          "description": "Our AI scans your video frame by frame, analyzing visual and audio cues to identify potential scene boundaries.",
          "details": [
            "Analyzes color histograms and motion vectors",
            "Detects audio level changes and silences",
            "Identifies shot boundaries and transitions"
          ]
        },
        "step2": {
          "title": "2. Scene Segmentation",
          "description": "The system groups related shots into coherent scenes based on visual and temporal relationships.",
          "details": [
            "Uses machine learning to understand scene context",
            "Considers shot duration and transition types",
            "Merges related shots into logical scenes"
          ]
        },
        "step3": {
          "title": "3. Content Classification",
          "description": "Each detected scene is analyzed and categorized based on its visual and audio content.",
          "details": [
            "Identifies key visual elements and objects",
            "Classifies scene type (e.g., interview, action, landscape)",
            "Analyzes audio for speech, music, and effects"
          ]
        },
        "step4": {
          "title": "4. Metadata Generation",
          "description": "Comprehensive metadata is generated for each scene, enabling powerful search and organization.",
          "details": [
            "Creates timestamps and duration information",
            "Generates representative thumbnails",
            "Extracts key dialogue and audio cues"
          ]
        }
      }
    },
    "whyItWorks": {
      "title": "Why Our Scene Detection Works So Well",
      "points": "<li>Our AI analyzes both <strong>visual and audio cues</strong> to identify scene boundaries with over 95% accuracy.</li><li>Advanced <strong>machine learning models</strong> understand context, not just visual changes, for more natural scene detection.</li><li>Real-time processing allows for <strong>immediate feedback</strong> and adjustments during filming or editing.</li>"
    },
    "recommendedStyle": {
      "title": "Recommended Style for Best Results",
      "description": "To get the most accurate scene detection, we recommend the following style guidelines:",
      "points": [
        "Maintain consistent lighting within each scene",
        "Use clear audio markers or clapperboards between takes",
        "Keep camera movements smooth and intentional",
        "Maintain consistent audio levels within scenes"
      ]
    },
    "title": "From Raw Footage to Storyboards: AI-Powered Video Analysis",
    "description": "Discover how AI transforms raw video into structured storyboards with automated scene detection and analysis.",
    "timeline": {
      "title": "Scene Timeline",
      "viewScene": "View Scene {1}"
    },
    "cta": "ğŸš€ Ready to bring your storyboard to life with AI? <strong>We can provide a full starter kit</strong> with example JSONs, ComfyUI workflows, and ffmpeg/AE templates to help you get started.",
    "sceneBreakdown": {
      "title": "Understanding Scene Detection Output",
      "introduction": "Let's break down a typical scene detection output to understand how our AI analyzes and structures video content. Below each explanation, you'll find the corresponding JSON structure that powers these insights.",
      "scenes": [
        {
          "title": "1. Scene Identification",
          "content": "Each scene is assigned a unique identifier and timestamp range, allowing for precise navigation through the video content. This forms the foundation of our analysis.",
          "example": "Scene 1 (00:00:02.50 - 00:00:05.20)",
          "jsonExample": {
            "scene_id": "scene_001",
            "start_time": 2.5,
            "end_time": 5.2,
            "duration": 2.7,
            "keyframe_index": 5,
            "keyframe_time": 3.8
          },
          "jsonDescription": "This JSON structure shows the basic identification data for a scene, including its unique ID, timing information, and the index/time of its representative keyframe."
        },
        {
          "title": "2. Visual Analysis",
          "content": "Our AI examines keyframes to understand the visual composition of each scene, including dominant colors, lighting conditions, and visual elements.",
          "example": "Keyframe analysis: Outdoor, daylight, multiple subjects",
          "jsonExample": {
            "visual_analysis": {
              "brightness": 0.78,
              "contrast": 0.65,
              "color_palette": [
                "#3A5FCD",
                "#87CEEB",
                "#F5F5DC"
              ],
              "dominant_colors": [
                {
                  "color": "#3A5FCD",
                  "percentage": 0.45
                },
                {
                  "color": "#87CEEB",
                  "percentage": 0.35
                },
                {
                  "color": "#F5F5DC",
                  "percentage": 0.2
                }
              ],
              "lighting_condition": "daylight",
              "environment": "outdoor",
              "detected_objects": [
                {
                  "label": "person",
                  "confidence": 0.97,
                  "count": 2
                },
                {
                  "label": "sky",
                  "confidence": 0.99,
                  "count": 1
                }
              ]
            }
          },
          "jsonDescription": "This JSON shows the visual analysis data, including color information, lighting conditions, and detected objects with confidence scores."
        },
        {
          "title": "3. Shot Composition",
          "content": "Within each scene, we identify individual shots and their transitions, helping understand the visual flow and pacing of the content.",
          "example": "3 shots detected with smooth cuts and one cross-fade",
          "jsonExample": {
            "shots": [
              {
                "shot_id": "shot_001",
                "start_time": 2.5,
                "end_time": 3.1,
                "transition": {
                  "type": "cut",
                  "confidence": 0.98
                },
                "camera_motion": {
                  "type": "static",
                  "confidence": 0.92
                }
              },
              {
                "shot_id": "shot_002",
                "start_time": 3.1,
                "end_time": 4.3,
                "transition": {
                  "type": "fade",
                  "duration": 0.3,
                  "confidence": 0.95
                },
                "camera_motion": {
                  "type": "pan_left",
                  "confidence": 0.88
                }
              }
            ]
          },
          "jsonDescription": "This JSON structure details the shot composition within a scene, including timing, transition types, and camera motion analysis."
        },
        {
          "title": "4. Content Classification",
          "content": "Scenes are automatically categorized based on their content, making it easy to find specific types of footage later.",
          "example": "Category: Drama, Setting: Ship Deck, Subjects: Main Characters",
          "jsonExample": {
            "content_analysis": {
              "primary_category": "drama",
              "secondary_categories": [
                "romance",
                "disaster"
              ],
              "setting": {
                "type": "ship_deck",
                "time_of_day": "night",
                "confidence": 0.92
              },
              "subjects": [
                {
                  "type": "main_character",
                  "name": "Jack",
                  "position": "center_frame",
                  "emotion": "determined",
                  "confidence": 0.89
                },
                {
                  "type": "main_character",
                  "name": "Rose",
                  "position": "center_frame",
                  "emotion": "fearful",
                  "confidence": 0.91
                }
              ],
              "sentiment": {
                "overall": "intense_dramatic",
                "confidence": 0.88,
                "emotions": [
                  "fear",
                  "determination",
                  "urgency"
                ]
              },
              "key_elements": [
                "lifeboat",
                "ocean",
                "moonlight"
              ],
              "narrative_importance": 0.95,
              "action_required": true
            }
          },
          "jsonDescription": "This JSON shows how the AI analyzes and classifies movie scenes, including character emotions, setting details, and narrative importance, with Titanic's dramatic lifeboat scene as an example."
        }
      ],
      "analysisTitle": "Putting It All Together",
      "analysisContent": "By combining these elements, our system creates a comprehensive map of your video content. This structured data powers features like intelligent search, automated editing, and content analysis.",
      "fullExampleTitle": "Complete Scene Data Example",
      "fullExampleDescription": "Here's how all the pieces come together in a complete scene analysis:",
      "fullExample": {
        "scene_id": "scene_001",
        "start_time": 2.5,
        "end_time": 5.2,
        "duration": 2.7,
        "metadata": {
          "created_at": "2025-12-11T14:25:30Z",
          "video_source": "interview_001.mp4",
          "resolution": "1920x1080",
          "fps": 30
        },
        "visual_analysis": {
          "brightness": 0.78,
          "contrast": 0.65,
          "color_palette": [
            "#3A5FCD",
            "#87CEEB",
            "#F5F5DC"
          ],
          "lighting_condition": "daylight",
          "environment": "studio"
        },
        "audio_analysis": {
          "has_speech": true,
          "speech_confidence": 0.92,
          "background_noise_level": 0.15,
          "speaker_gender": [
            "male",
            "female"
          ],
          "speech_text": "Let's discuss how AI is transforming video production..."
        },
        "content_analysis": {
          "primary_category": "interview",
          "setting": "studio",
          "subjects": [
            "host",
            "guest"
          ],
          "sentiment": "neutral_positive"
        },
        "shots": [
          {
            "shot_id": "shot_001",
            "start_time": 2.5,
            "end_time": 3.1,
            "keyframe": "https://example.com/keyframes/scene_001_shot_001.jpg",
            "transition": {
              "type": "cut",
              "confidence": 0.98
            }
          },
          {
            "shot_id": "shot_002",
            "start_time": 3.1,
            "end_time": 5.2,
            "keyframe": "https://example.com/keyframes/scene_001_shot_002.jpg",
            "transition": {
              "type": "fade",
              "confidence": 0.95
            }
          }
        ]
      },
      "benefitsTitle": "Key Benefits",
      "benefits": [
        "<strong>Efficient Editing:</strong> Jump directly to any scene or shot without scrubbing through hours of footage",
        "<strong>Smart Search:</strong> Find content based on visual elements, not just metadata",
        "<strong>Consistent Quality:</strong> Identify and maintain visual consistency across your project",
        "<strong>Data-Driven Decisions:</strong> Get insights into your content structure and pacing"
      ]
    },
    "conclusion": {
      "title": "Transforming Video Production with AI",
      "p1": "AI-powered scene detection is revolutionizing how we approach video production. By automating the tedious process of scene identification and organization, creators can focus on what truly matters â€“ telling compelling stories. Our technology bridges the gap between raw footage and polished content, making professional-grade video analysis accessible to everyone.",
      "p2": "As we continue to refine our algorithms and expand our capabilities, we're excited to see how filmmakers, educators, and content creators will leverage these tools to push the boundaries of visual storytelling. The future of video production is here, and it's more efficient and creative than ever before."
    },
    "richStructuredOutput": "Rich, Structured Output",
    "richStructuredOutputText": "Our system generates comprehensive storyboard data with detailed metadata for each scene, giving you complete control over your video content.",
    "advancedUsage": {
      "title": "Advanced Usage & Customization",
      "description": "The scene detection system is highly customizable to fit different use cases. Here are some advanced features and customization options:",
      "features": {
        "thresholds": {
          "title": "Custom Scene Detection Thresholds",
          "description": "Adjust the sensitivity of scene detection by modifying the threshold parameter. Lower values make the detection more sensitive to changes."
        },
        "aiAnalysis": {
          "title": "AI-Enhanced Analysis",
          "description": "Enable AI analysis for more detailed scene understanding and labeling. This requires additional setup with the Ollama server."
        },
        "outputCustomization": {
          "title": "Output Customization",
          "description": "Customize the output format and include additional metadata in the generated storyboard."
        }
      },
      "integration": {
        "title": "Integration with Other Tools",
        "description": "The storyboard output can be easily integrated with other tools and workflows. Here are some examples:",
        "tools": {
          "editingSoftware": {
            "title": "Video Editing Software",
            "description": "Import the JSON output into video editors that support script-based editing"
          },
          "cms": {
            "title": "Content Management Systems",
            "description": "Automatically generate metadata for video assets"
          },
          "aiTraining": {
            "title": "AI Training Data",
            "description": "Use the structured output as training data for machine learning models"
          }
        }
      }
    },
    "Benefitss": {
      "title": "Benefits",
      "description": "The benefits of using our AI-powered scene detection system include:",
      "tags": {
        "computerVision": "Computer Vision",
        "deepLearning": "Deep Learning",
        "realTimeAnalysis": "Real-time Analysis"
      },
      "features": {
        "modularArchitecture": {
          "title": "Modular Architecture",
          "description": "The system is built with separate components for video analysis, AI processing, and output generation, making it easy to extend and maintain."
        },
        "performance": {
          "title": "Performance Optimized",
          "description": "Efficient frame processing and parallelization ensure fast analysis even for long videos."
        },
        "aiEnhanced": {
          "title": "AI-Enhanced Analysis",
          "description": "Optional AI components provide deeper scene understanding and more accurate labeling."
        }
      }
    },
    "benefits": {
      "subtitle": "WHY CHOOSE OUR SOLUTION",
      "title": "The Power of AI-Powered Scene Analysis"
    },
    "integration": {
      "title": "Easy Integration",
      "description": "The structured JSON output makes it easy to integrate with other tools and workflows:",
      "technologies": [
        "Python",
        "JavaScript",
        "Node.js",
        "React",
        "Vue"
      ]
    },
    "features10": {
      "comprehensiveData": {
        "title": "Comprehensive Scene Data",
        "items": [
          "Precise timing information for each scene",
          "Key frames for visual reference",
          "Object detection results",
          "Camera movement analysis"
        ]
      },
      "export": {
        "title": "Export Option",
        "formats": {
          "json": "JSON"
        }
      }
    },
    "performance": {
      "title": "Performance Optimized",
      "features": {
        "speed": "5-10x faster than real-time",
        "memory": "Low memory footprint",
        "parallel": "Parallel processing"
      }
    }
  },
  "nanoBananaProPrompts": {
    "title": "Nano Banana Pro Prompts",
    "description": "Discover a collection of creative and effective prompts for your AI content generation needs. These prompts are designed to help you get the most out of your AI tools.",
    "prompts": {
      "title": "Popular Nano Banana Pro Prompts",
      "prompt1": "Create a short story about a futuristic city where emotions are currency",
      "prompt2": "Generate a dialogue between two characters meeting for the first time in a coffee shop",
      "prompt3": "Write a product description for an innovative AI-powered kitchen gadget"
    }
  },
  "qaBotToTaskAgent": {
    "heading": "ğŸ—ï¸ From QA Bot to Task Agent: An Architecture Guide",
    "description": "Learn how to build reliable task agents that go beyond simple question answering",
    "tldr": "TL;DR: Stop building chatbots that just answer questions. Start building Task Agents that actually do work.",
    "intro": "This guide explains the architectural shift from monolithic QA bots to Task Agents using Static Rules, Dynamic Skills, and Deterministic Hooksâ€”with concrete code examples and open-source references.",
    "coreShift": {
      "title": "1. The Core Shift: QA Bot â†’ Task Agent",
      "content": [
        "Most AI systems today are still context-stuffed QA bots:",
        "â€¢ They answer questions well",
        "â€¢ They hallucinate under pressure",
        "â€¢ They lack guarantees around execution, safety, and consistency"
      ],
      "keyInsight": "The key insight: Don't scale context. Structure it."
    },
    "threeLayerArch": {
      "title": "2. The Three-Layer Architecture",
      "layers": [
        {
          "title": "ğŸ§± 1. Static Context â€” Rules (Always On)",
          "points": [
            "Mental model: Employee handbook",
            "Always loaded",
            "Defines identity, coding standards, behavioral constraints",
            "Prevents hallucinations and style drift",
            "Small, stable, human-editable"
          ]
        },
        {
          "title": "ğŸ› ï¸ 2. Dynamic Context â€” Skills (On Demand)",
          "points": [
            "Mental model: Toolbox",
            "Loaded only when needed",
            "Each skill is a self-contained capability",
            "Keeps the context window clean"
          ]
        },
        {
          "title": "âš“ 3. Deterministic Hooks â€” Guardrails",
          "points": [
            "Mental model: Security + Compliance layer",
            "Not probabilistic",
            "Runs before / after LLM reasoning",
            "Enforces rules that must never fail"
          ]
        }
      ]
    },
    "projectStructure": {
      "title": "3. Recommended Project Structure",
      "structure": "my-task-agent/\nâ”œâ”€â”€ .cursorrules\nâ”œâ”€â”€ main.py\nâ”œâ”€â”€ tools/\nâ”‚   â””â”€â”€ linear_mcp.py\nâ””â”€â”€ README.md"
    },
    "staticContextExample": {
      "title": "4. Static Context Example: .cursorrules",
      "content": "# ROLE\nYou are a Senior Python Engineer focused on production-grade systems.\n\n# RULES\n- NEVER use print() for debugging\n- ALWAYS type-hint functions\n- Propose a plan if touching >3 files\n\n# BEHAVIOR\n- Be concise\n- Ask clarifying questions if needed\n\nReference: https://github.com/PatrickJS/awesome-cursorrules"
    },
    "dynamicSkillExample": {
      "title": "5. Dynamic Skill Example (MCP)",
      "content": "from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"DevTools\")\n\n@mcp.tool()\ndef create_linear_ticket(title: str, priority: str = \"low\") -> str:\n    ticket_id = f\"LIN-{hash(title) % 10000}\"\n    return f\"Created ticket {ticket_id} with priority={priority}\"\n\nif __name__ == \"__main__\":\n    mcp.run()\n\nReference: https://github.com/modelcontextprotocol/python-sdk"
    },
    "deterministicHookExample": {
      "title": "6. Deterministic Hook Example",
      "content": "def compliance_check_hook(state):\n    user_input = state[\"messages\"][-1].content.lower()\n    if \"password\" in user_input or \"api_key\" in user_input:\n        return {\"error\": \"Security violation detected\"}\n    return agent_node(state)\n\nReference: https://langchain-ai.github.io/langgraph/"
    },
    "finalThought": "If your agent only answers questions, it's a chatbot. If it reliably executes work, it's a Task Agent."
  },
  "ageAi": {
    "heading": "Data Science in the Age of AI: Is the \"Sexiest Job\" Still Sexy?",
    "intro1": "\"It was the best of times, it was the worst of times.\"",
    "intro2": "A decade ago, Harvard Business Review called Data Scientist the \"sexiest job of the 21st century.\" Today, the landscape is shifting beneath our feet. While the demand for data talent remains high, the rise of Generative AI is fundamentally changing what it means to be a 'Data Scientist.'",
    "realityCheck": {
      "title": "The Reality Check",
      "content": "Traditional domains like Search, Ads, and Recommendation (SAR) are maturing, and the industry is shifting its focus toward heavy-duty engineering and AI architecture. We are seeing a strange paradox.",
      "lowBarTrap": {
        "title": "The \"Low-Bar\" Trap",
        "content": "Master's students can now use GPT-4 to handle data cleaning, EDA, and visualization in seconds. However, without a solid foundation, they often lack the judgment to know when the AI is \"hallucinating\" or providing statistically flawed results."
      },
      "stakeholderShift": {
        "title": "Stakeholder Shift",
        "content": "When business partners can write their own prompts to get basic insights, many DS professionals feel \"under-stimulated,\" trapped in endless meetings and repetitive prompt engineering."
      }
    },
    "strategicPillars": {
      "title": "How to Stay Indispensable: Two Strategic Pillars",
      "intro": "To thrive in this era, we need to evolve from 'builders of models' to 'architects of value.' I see this happening in two dimensions:",
      "buildingTools": {
        "title": "Building the Tools (The Engineer/Architect Mindset)",
        "subtitle": "Don't just use the AI; improve it.",
        "points": [
          {
            "title": "Model Evaluation & Governance",
            "content": "As AI output becomes a commodity, the person who can define what a 'good' result looks like is the most valuable person in the room. Focus on specialized evaluation frameworks (like risk-weighting in Finance)."
          },
          {
            "title": "Domain Fine-Tuning",
            "content": "Mastering techniques like LoRA or RAG to inject specific business knowledge into LLMs."
          },
          {
            "title": "Automation",
            "content": "Lead internal initiatives like 'Virtual Analysts' or automated experimentation pipelines."
          }
        ]
      },
      "leveragingTools": {
        "title": "Leveraging the Tools (The Strategist Mindset)",
        "subtitle": "Use AI to 10x your output so you can focus on what humans do best.",
        "points": [
          {
            "title": "Domain Expertise",
            "content": "AI knows the 'how,' but you know the 'why.' Deep business understanding allows you to provide the right context that AI lacks."
          },
          {
            "title": "Critical Thinking & Experimentation",
            "content": "While AI can generate code, human DS skills are still core for hypothesis testing, causal inference, and interpreting 'messy' real-world data."
          },
          {
            "title": "Communication & Influence",
            "content": "The ability to translate complex data into a business story and build stakeholder trust is a 'soft' skill that has become a 'hard' requirement."
          }
        ]
      }
    },
    "bottomLine": {
      "title": "The Bottom Line",
      "content1": "AI hasn't killed Data Science; it has raised the floor. If your value was purely in writing SQL or tuning hyperparameters, the 'sexiness' is fading. But if you can bridge the gap between business problems and AI solutions, your value has never been higher.",
      "content2": "Personal experience is your edge. A LLM can mimic logic, but it doesn't have the years of 'battle scars' from failed deployments or the intuition built from navigating complex organizations."
    },
    "discussion": {
      "title": "Let's discuss:",
      "prompt": "Are you feeling more 'efficient' or 'replaced' in your current role? How are you evolving your toolkit this year?"
    },
    "hashtags": [
      "#DataScience",
      "#AI",
      "#MachineLearning",
      "#GenerativeAI",
      "#CareerDevelopment",
      "#TechTrends"
    ]
  }
}